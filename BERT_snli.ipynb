{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_snli.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c47a436a2c564f4a81249485e50fed6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ca6e7874d6cb44508070e46509295006",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c479c0d3d51344c8b7ca1b1f9944f3a6",
              "IPY_MODEL_a3512677775b4fe084694d72c3b8770d"
            ]
          }
        },
        "ca6e7874d6cb44508070e46509295006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c479c0d3d51344c8b7ca1b1f9944f3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6560efb5ab3a491891f28534e0bfaa81",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f65e637b4d9141c09726a60d6dcc2be5"
          }
        },
        "a3512677775b4fe084694d72c3b8770d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_22262866fae348bbb26cd12cb322897f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:10&lt;00:00, 21.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_093981eb84e846a0991c35e04d27babb"
          }
        },
        "6560efb5ab3a491891f28534e0bfaa81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f65e637b4d9141c09726a60d6dcc2be5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22262866fae348bbb26cd12cb322897f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "093981eb84e846a0991c35e04d27babb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PWRp6JI3U45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "1182375b-7889-4283-db5a-700b851e5141"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "\n",
        "!pip install transformers\n",
        "!pip install wget\n",
        "!pip install jsonlines\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wget\n",
        "import json\n",
        "import jsonlines\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 312kB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5a15d4d348b780e71947e1ea7a21164430f5d83f953e159be7435d2e57116cbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=e23e76f1294afcddd062f58fdf7080bc5627b70eb4732da0fdeb93fec639fae3\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHnvOe9U4v8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf594415-da21-4d57-e49d-099a27d29b04"
      },
      "source": [
        "#########   Setting GPU device   #########\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(device)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S54BrfEB3nIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "9ba3589f-eec0-4cd0-94ee-aae763d96487"
      },
      "source": [
        "##########   Download SNLI dataset   ############\n",
        "\n",
        "snli_url = \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\" \n",
        "if not os.path.exists('./snli_1.0.zip'):\n",
        "    wget.download(snli_url, './snli_1.0.zip')\n",
        "\n",
        "# Unzip the dataset\n",
        "if not os.path.exists('./snli_1.0/'):\n",
        "    !unzip snli_1.0.zip\n",
        "\n",
        "def processed_snli_data(path):\n",
        "    hyp_prem = []\n",
        "    labels = []\n",
        "    with jsonlines.open(path, \"r\") as f:\n",
        "        for line in f.iter():\n",
        "            json_string = json.dumps(line)\n",
        "            ex = json.loads(json_string)\n",
        "            if ex['gold_label'] != \"-\":\n",
        "                hyp_prem.append(ex['sentence1'] + \" \" + ex['sentence2'])\n",
        "                labels.append(ex['gold_label'])\n",
        "\n",
        "    # hyp_prem = hyp_prem[:160000]\n",
        "    # labels = labels[:160000]\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
        "                                            do_lower_case=True)\n",
        "    \n",
        "    print(hyp_prem[0])\n",
        "    print(tokenizer.tokenize(hyp_prem[0]))\n",
        "\n",
        "    sent = []\n",
        "    for sentence in hyp_prem:\n",
        "        s = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "        sent.append(s)\n",
        "\n",
        "    print(sent[0])\n",
        "    return sent, labels\n",
        "\n",
        "def snli_dataloader(sentence, lab, turn):\n",
        "    max_len = 112\n",
        "    labels = []\n",
        "    for y in lab:\n",
        "        if y == 'neutral':\n",
        "            labels.append(0)\n",
        "        elif y == 'entailment':\n",
        "            labels.append(1)\n",
        "        elif y == 'contradiction':\n",
        "            labels.append(2)\n",
        "\n",
        "    sentence = pad_sequences(sentence, maxlen=max_len, dtype=\"long\",\n",
        "                        value=0, truncating=\"post\", padding=\"post\")\n",
        "    print(sentence[0])\n",
        "    \n",
        "    mask = []\n",
        "    for s in sentence:\n",
        "        m = [int(word_id > 0) for word_id in s]\n",
        "        mask.append(m)\n",
        "    print(mask[2])  # contains 1 for original words, 0 for padded words\n",
        "    test_size = 0.1 if turn == \"train\" else 0.01\n",
        "\n",
        "    train_set, test_set, train_labels, test_labels = train_test_split(\n",
        "        sentence, labels, random_state=2018, test_size=test_size\n",
        "    )\n",
        "    \n",
        "    train_mask, test_mask, _, _ = train_test_split(mask, labels, \n",
        "                                                random_state=2018, \n",
        "                                                test_size=test_size)\n",
        "\n",
        "    print(\"train set size\",len(train_set), len(train_labels))  # 0.6 times original num of sentences\n",
        "    print(\"test set size\", len(test_set))   # 0.4 times original num of sentences\n",
        "    print()\n",
        "\n",
        "    ########   convert NumPy arrays to Tensor data  ########\n",
        "    print(train_set.shape)\n",
        "    print(train_labels[:10])\n",
        "    train_labels = np.array(train_labels)\n",
        "    test_labels = np.array(test_labels)\n",
        "\n",
        "    train_set = torch.tensor(train_set)\n",
        "    test_set = torch.tensor(test_set)\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    test_labels = torch.tensor(test_labels)\n",
        "    train_mask = torch.tensor(train_mask)\n",
        "    test_mask = torch.tensor(test_mask)\n",
        "\n",
        "    print(\"train data shape\", train_set.shape)\n",
        "    print(\"test data shape\", train_labels.shape)\n",
        "    print(\"train mask shape\", train_mask.shape)\n",
        "\n",
        "    #######   Dataloader to load train and test data in batches  ########\n",
        "\n",
        "    batch_size = 32  # Recommended in paper\n",
        "    train = TensorDataset(train_set, train_mask, train_labels)\n",
        "    sampler = RandomSampler(train)\n",
        "    train_loader = DataLoader(train, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "    test = TensorDataset(test_set, test_mask, test_labels)\n",
        "    samp = RandomSampler(test)\n",
        "    test_loader = DataLoader(test, batch_size=batch_size, sampler=samp)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny_oP1OR4KgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494,
          "referenced_widgets": [
            "c47a436a2c564f4a81249485e50fed6e",
            "ca6e7874d6cb44508070e46509295006",
            "c479c0d3d51344c8b7ca1b1f9944f3a6",
            "a3512677775b4fe084694d72c3b8770d",
            "6560efb5ab3a491891f28534e0bfaa81",
            "f65e637b4d9141c09726a60d6dcc2be5",
            "22262866fae348bbb26cd12cb322897f",
            "093981eb84e846a0991c35e04d27babb"
          ]
        },
        "outputId": "594ea371-1046-40e0-bcd5-5354ebbc62c7"
      },
      "source": [
        "train_sent, labels = processed_snli_data(\"./snli_1.0/snli_1.0_train.jsonl\")\n",
        "print(len(train_sent))\n",
        "print('Max sentence length: ', max([len(sen) for sen in train_sent]))\n",
        "snli_trainloader, _ = snli_dataloader(train_sent, labels, \"train\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c47a436a2c564f4a81249485e50fed6e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A person on a horse jumps over a broken down airplane. A person is training his horse for a competition.\n",
            "['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.', 'a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.']\n",
            "[101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 1037, 2711, 2003, 2731, 2010, 3586, 2005, 1037, 2971, 1012, 102]\n",
            "549367\n",
            "Max sentence length:  124\n",
            "[  101  1037  2711  2006  1037  3586 14523  2058  1037  3714  2091 13297\n",
            "  1012  1037  2711  2003  2731  2010  3586  2005  1037  2971  1012   102\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "train set size 494430 494430\n",
            "test set size 54937\n",
            "\n",
            "(494430, 112)\n",
            "[1, 1, 2, 2, 1, 2, 0, 1, 1, 0]\n",
            "train data shape torch.Size([494430, 112])\n",
            "test data shape torch.Size([494430])\n",
            "train mask shape torch.Size([494430, 112])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmRc_rBp4m3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######   define the model   ###########\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "\n",
        "bert_model = bert_model.to(device)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vswLPN2D49RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########  define optimizer, write accuracy fn  ###########\n",
        "\n",
        "optimizer = AdamW(bert_model.parameters(), lr=2e-5, eps=1e-8)\n",
        "num_epochs = 4\n",
        "loss_arr = []\n",
        "\n",
        "def compute_accuracy(preds, targets):\n",
        "    return (torch.argmax(preds, dim=1) == targets).float().mean().item()\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6SYEVyj5Dmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############   TRAINING OF BERT MODEL  ##################\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    bert_model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, data in enumerate(snli_trainloader):\n",
        "        batch_data = data[0].to(device).long()\n",
        "        batch_mask = data[1].to(device).long()\n",
        "        batch_labels = data[2].to(device).long()\n",
        "\n",
        "        bert_model.zero_grad()\n",
        "        pred = bert_model(batch_data,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=batch_mask,\n",
        "                             labels=batch_labels)\n",
        "        \n",
        "        # As we call the model with labels, it returns the loss in a tuple\n",
        "        loss = pred[0]  \n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()  # Backprpagation\n",
        "\n",
        "        # Clip Gradient norm to mitigate exploding of gradients\n",
        "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss /= len(snli_trainloader)\n",
        "    print(\"train loss after %d epochs is %f \" %(ep+1, epoch_loss))\n",
        "    loss_arr.append(epoch_loss)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzk21awU5aAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "6821a0d4-92e0-403a-b681-9422d23cceec"
      },
      "source": [
        "##############  Load test data  ####################\n",
        "\n",
        "test_sent, labels = processed_snli_data(\"./snli_1.0/snli_1.0_test.jsonl\")\n",
        "print(len(test_sent))\n",
        "print('Max sentence length: ', max([len(sen) for sen in test_sent]))\n",
        "snli_testloader, _ = snli_dataloader(test_sent, labels, \"test\")\n",
        "\n",
        "print(len(snli_testloader))\n",
        "test_acc = 0.0\n",
        "steps = 0\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This church choir sings to the masses as they sing joyous songs from the book at a church. The church has cracks in the ceiling.\n",
            "['this', 'church', 'choir', 'sings', 'to', 'the', 'masses', 'as', 'they', 'sing', 'joy', '##ous', 'songs', 'from', 'the', 'book', 'at', 'a', 'church', '.', 'the', 'church', 'has', 'cracks', 'in', 'the', 'ceiling', '.']\n",
            "[101, 2023, 2277, 6596, 10955, 2000, 1996, 11678, 2004, 2027, 6170, 6569, 3560, 2774, 2013, 1996, 2338, 2012, 1037, 2277, 1012, 1996, 2277, 2038, 15288, 1999, 1996, 5894, 1012, 102]\n",
            "9824\n",
            "Max sentence length:  75\n",
            "[  101  2023  2277  6596 10955  2000  1996 11678  2004  2027  6170  6569\n",
            "  3560  2774  2013  1996  2338  2012  1037  2277  1012  1996  2277  2038\n",
            " 15288  1999  1996  5894  1012   102     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "train set size 9725 9725\n",
            "test set size 99\n",
            "\n",
            "(9725, 112)\n",
            "[1, 2, 2, 1, 0, 2, 1, 1, 1, 1]\n",
            "train data shape torch.Size([9725, 112])\n",
            "test data shape torch.Size([9725])\n",
            "train mask shape torch.Size([9725, 112])\n",
            "304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep9WT0-Z5gjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########   Performance over Test set  ##############\n",
        "\n",
        "bert_model.eval()\n",
        "for batch in snli_testloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    batch_data, batch_mask, batch_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = bert_model(batch_data,\n",
        "                                token_type_ids=None,\n",
        "                                attention_mask=batch_mask)\n",
        "\n",
        "    logits = preds[0]\n",
        "    logits = logits.detach().cpu()\n",
        "    targets = batch_labels.to('cpu')\n",
        "    \n",
        "    acc = compute_accuracy(logits, targets)\n",
        "    test_acc += acc\n",
        "    steps += 1\n",
        "\n",
        "print(\"final test set accuracy is \", (test_acc / steps))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}