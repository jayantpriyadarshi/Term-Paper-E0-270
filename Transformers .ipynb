{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYYCmq8T10Pm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3347d2a9-cdb0-43ac-a532-33b5e1eba462"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchtext import data, datasets\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import spacy\n",
        "import math, copy, time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcZlfOm-2mjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):  # N = 6 in the paper\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "def attention(q, k, v, mask=None):\n",
        "    d_k = q.size(-1)\n",
        "    sc = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        sc = sc.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    att = F.softmax(sc, dim=-1)\n",
        "    return torch.matmul(att, val), att\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOw4VQquSniG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAtt(nn.Module):\n",
        "    def __init__(self, reduction_factor, d_model):      # h = 8 as per paper\n",
        "        super(MultiHeadAtt, self).__init__()\n",
        "        self.d_k = d_model // h\n",
        "        self.h = reduction_factor\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.l_transforms = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.att = None\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        \n",
        "        query, key, value =  [linear(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for linear, x in zip(self.l_transforms, (query, key, value))]\n",
        "        \n",
        "        # Apply attention on all the projected vectors in batch.\n",
        "        x, self.att = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # \"Concat\" using a view and apply a final linear. \n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.l_transforms[-1](x)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iFIDtX_AaZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, f):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.ones = nn.Parameter(torch.ones(f))\n",
        "        self.zeros = nn.Parameter(torch.zeros(f))\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = x.mean(-1, keepdim=True)\n",
        "        sigma = x.std(-1, keepdim=True)\n",
        "        return self.ones * (x - mu) / (sigma + self.eps) + self.zeors\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rScE_VDMoXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ApplyNorm(nn.Module):\n",
        "    def __init__(self, size, dropout):\n",
        "        super(ApplyNorm, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.regularizer = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sub_layer):\n",
        "        return x + self.regularizer(sub_layer(self.norm(x)))\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OdcaM6CRdfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, size, layer1, layer2, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attention = layer1\n",
        "        self.fnn = layer2\n",
        "        self.sublayer = clones(ApplyNorm(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attention(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.fnn)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0lQ95sY-BjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssn4LRY9ZADi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, layer1, layer2, layer3, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = layer1\n",
        "        self.ed_attention =layer2\n",
        "        self.fnn = layer3\n",
        "        self.sublayer = clones(ApplyNorm(size, dropout), 3)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, prev, mask1, mask2):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attention(x, x, x, mask2))\n",
        "        x = self.sublayer[1](x, lambda x: self.ed_attention(x, prev, prev, mask1))\n",
        "        return self.sublayer[2](x, self.fnn)\n",
        "        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iycRTNe0crst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, prev, mask1, mask2):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, prev, mask1, mask2)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOPlLhb7kTVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Positional_FNN(nn.Module):\n",
        "    def __init__(self, d_model, d_fnn):\n",
        "        super(Positional_FNN, self).__init__()\n",
        "        self.layer1 = nn.Linear(d_model, d_fnn)\n",
        "        self.layer2 = nn.Linear(d_fnn, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = F.relu(x)\n",
        "        return self.layer2(x)\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VkqRmXRdrE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedd(nn.Module):\n",
        "    def __init__(self, vocabulary, d_model):  # d_model = 512 in Paper\n",
        "        super(Embed, self).__init__()\n",
        "        self.vector = nn.Embedding(vocabulary, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vector(x) * math.sqrt(self.d_model)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4dwz681BdUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia1hhkVx7Q7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, enc, dec, src_emb, tgt_emb, gen):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = enc\n",
        "        self.decoder = dec\n",
        "        self.source_embedding = src_emb\n",
        "        self.target_embedding = tgt_emb\n",
        "        self.final = gen\n",
        "\n",
        "    def compute_encoding(self, src, src_mask):\n",
        "        src_embeddings = self.source_embedding(src)\n",
        "        return self.encoder(src_embeddings, src_mask)\n",
        "\n",
        "    def compute_decoding(self, m, src_mask, tgt, tgt_mask):\n",
        "        tgt_embeddings = self.target_embedding(tgt)\n",
        "        return self.decoder(tgt_embeddings, m, src_mask, tgt_mask)\n",
        "\n",
        "    def forward(self, source, target, source_mask, target_mask):\n",
        "        encoding = self.compute_encoding(source, source_mask)\n",
        "        return self.compute_decoding(encoding, source_mask, target, target_mask)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzLI6bkp61Ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Final_Proj(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super(Final_Proj, self).__init__()\n",
        "        self.projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        return F.log_softmax(x, dim=-1)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN9NLMzr2aPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Transformer(source_voc, target_voc):\n",
        "    # Setting hyperparameters as per the Paper\n",
        "    N = 6\n",
        "    d_model = 512\n",
        "    d_fnn = 2048\n",
        "    h = 8\n",
        "    dropout = 0.1\n",
        "\n",
        "    att = MultiHeadAtt(reduction_factor=h, d_model=d_model)\n",
        "    fnn = Positional_FNN(d_model=d_model, d_fnn=d_fnn)\n",
        "    posn = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
        "    src_embeddings = Embedd(source_voc, d_model=d_model)\n",
        "    tgt_embeddings = Embedd(target_voc, d_model=d_model)\n",
        "\n",
        "    c = copy.deepcopy\n",
        "    encoder_layer = EncoderLayer(d_model, c(att), c(fnn), dropout)\n",
        "    encoder = Encoder(layer=encoder_layer, N=N)\n",
        "    decoder_layer = DecoderLayer(d_model, c(att), c(att), c(fnn), dropout)\n",
        "    decoder = Decoder(layer=decoder_layer, N=N)\n",
        "\n",
        "    transformer_model = Model(encoder, decoder, \n",
        "                              nn.Sequential(src_embeddings, c(posn)),\n",
        "                              nn.Sequential(tgt_embeddings, c(posn)), \n",
        "                              Final_Proj(d_model, target_voc))\n",
        "    \n",
        "    for param in transformer_model.parameters():\n",
        "        if param.dim() > 1:\n",
        "            nn.init.xavier_uniform(param)\n",
        "    \n",
        "    return transformer_model\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz5qXItkAvgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}